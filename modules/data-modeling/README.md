# Module C: Data Modeling & Cloud Ingestion

This module bridges Kafka streams and Snowflake analytics, including OSM ground truth ingestion and spatial proximity matching.

---

## üìÅ Directory Structure

```
modules/data-modeling/
‚îú‚îÄ‚îÄ snowflake/
‚îÇ   ‚îú‚îÄ‚îÄ 01_initial_setup.sql          # ‚úÖ Database, warehouse, resource monitor
‚îÇ   ‚îú‚îÄ‚îÄ 02_create_tables.sql          # ‚úÖ STG_DETECTIONS and REF_OSM_NODES
‚îÇ   ‚îú‚îÄ‚îÄ 03_configure_kafka_user.sql   # ‚úÖ Kafka connector authentication
‚îÇ   ‚îú‚îÄ‚îÄ 04_create_views.sql           # ‚úÖ Flattened views for dbt
‚îÇ   ‚îú‚îÄ‚îÄ 05_validation.sql             # ‚úÖ Health checks
‚îÇ   ‚îî‚îÄ‚îÄ 06_test_proximity_matching.sql # ‚úÖ Spatial join testing
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingest_osm_to_snowflake.py   # Upload OSM XML to Snowflake
‚îÇ   ‚îú‚îÄ‚îÄ download_osm_from_detections.py  # Auto-download OSM for detection area
‚îÇ   ‚îî‚îÄ‚îÄ README.md                     # Script documentation
‚îú‚îÄ‚îÄ requirements.txt                  # Python dependencies
‚îú‚îÄ‚îÄ .env.example                      # Environment variable template
‚îî‚îÄ‚îÄ README.md                         # This file
```

---

## üöÄ Quick Start

### Prerequisites

- Snowflake account (free trial: $400 credits)
- Confluent Cloud Kafka cluster (from Module B)
- Python 3.9+ with `snowflake-connector-python`

---

### Step 1: Snowflake Account Setup

#### 1.1 Create Free Trial

1. Go to [signup.snowflake.com](https://signup.snowflake.com)
2. Choose **Standard Edition**
3. Select **AWS** ‚Üí **US East (N. Virginia)** (lowest cost)
4. Activate 30-day trial ($400 credits)

#### 1.2 Initial Configuration

Run SQL scripts in **Snowflake Worksheets** (in order):

```sql
-- In Snowflake UI: Worksheets ‚Üí + ‚Üí SQL Worksheet

-- 1. Create database, warehouse, resource monitor
@snowflake/01_initial_setup.sql

-- 2. Create tables (STG_DETECTIONS, REF_OSM_NODES)
@snowflake/02_create_tables.sql

-- 3. Create flattened views
@snowflake/04_create_views.sql

-- 4. Run validation
@snowflake/05_validation.sql
```

**What this does**:
- ‚úÖ Creates `SENTINEL_MAP` database with `RAW` schema
- ‚úÖ Creates `SENTINEL_WH` warehouse (XSMALL, auto-suspend 60s)
- ‚úÖ Sets resource monitor: $50 credit quota with auto-suspend at 90%
- ‚úÖ Creates ingestion tables with GEOGRAPHY type for spatial queries

---

### Step 2: Kafka-Snowflake Connector (Optional)

**Skip this if testing locally.** This connects your Kafka stream from Module B to Snowflake.

<details>
<summary>Click to expand Kafka connector setup</summary>

#### 2.1 Configure Snowflake User

```sql
-- Run in Snowflake:
@snowflake/03_configure_kafka_user.sql
```

#### 2.2 Create Confluent Connector

1. Log into Confluent Cloud
2. Navigate to **Connectors** ‚Üí **Add Connector**
3. Search "Snowflake Sink"
4. Configure:
   - **Topics**: `sentinel_map_detections`
   - **Snowflake URL**: `<YOUR_ACCOUNT>.snowflakecomputing.com`
   - **Database**: `SENTINEL_MAP`
   - **Schema**: `RAW`
   - **Table**: `STG_DETECTIONS`
   - **Ingestion Method**: `SNOWPIPE_STREAMING`
   - **Error Tolerance**: `none` (fail fast, don't hide errors)

**Critical**: Set **Error Tolerance = none** to see connector errors immediately.

</details>

---

### Step 3: OSM Ground Truth Ingestion

#### 3.1 Install Dependencies

```bash
cd modules/data-modeling
pip install -r requirements.txt
```

#### 3.2 Configure Environment

```bash
# Copy template
cp .env.example .env

# Edit .env with your Snowflake credentials
nano .env
```

**Required settings**:
```bash
SNOWFLAKE_ACCOUNT=abc12345.us-east-1  # Find in Snowflake UI
SNOWFLAKE_USER=YOUR_USERNAME
SNOWFLAKE_PASSWORD=YOUR_PASSWORD
```

#### 3.3 Download OSM Data

**Option 1: Auto-download from Snowflake detections (Recommended)**
```bash
cd modules/data-modeling/scripts
python download_osm_from_detections.py
```
This queries Snowflake for your detection GPS bounds and downloads only relevant OSM nodes.

**Option 2: Download Toronto traffic infrastructure**
```bash
# Download via Overpass API (traffic signals, stop signs, etc.)
curl "https://overpass-api.de/api/interpreter" \
  --data-urlencode "data=[out:xml][timeout:300];
  area[name=\"Toronto\"]->.a;
  (
    node[\"highway\"=\"traffic_signals\"](area.a);
    node[\"highway\"=\"stop\"](area.a);
    node[\"traffic_sign\"](area.a);
  );
  out body;" > ../../data/toronto_traffic.xml
```

#### 3.4 Run Ingestion

```bash
# Upload OSM XML to Snowflake
python scripts/ingest_osm_to_snowflake.py
```

**Expected output**:
```
üìñ Parsing /path/to/local-mvp/osm.xml...
‚úÖ Found 51556 traffic infrastructure nodes
üöÄ Uploading to Snowflake...
‚úÖ Inserted batch 1/513

üìä Upload Summary:
   Total nodes: 51556
   Unique types: 12

‚ú® OSM ingestion complete!
```

---

### Step 4: Test Spatial Proximity Matching

```sql
-- Run in Snowflake:
@snowflake/06_test_proximity_matching.sql
```

**What this does**:
1. Extracts detection GPS from VARIANT JSON
2. Converts to GEOGRAPHY points
3. Joins with OSM nodes within 50 meters
4. Shows matched detections with distance

**Expected results**:
```
Total Detections: 20
Detections with Matches: 11
Match Rate: 55%
Avg Matches per Detection: 0.85
```

---

## üîë Key Features

### Table Schemas

#### STG_DETECTIONS (Kafka Ingestion)
```sql
CREATE TABLE STG_DETECTIONS (
    RECORD_METADATA VARIANT,        -- Kafka metadata
    RECORD_CONTENT VARIANT,         -- Raw JSON from Module B
    INGESTED_AT TIMESTAMP_NTZ,
    KAFKA_TOPIC VARCHAR(255),
    KAFKA_PARTITION NUMBER(10,0),
    KAFKA_OFFSET NUMBER(38,0),
    KAFKA_TIMESTAMP TIMESTAMP_NTZ
);
```

**JSON Schema in RECORD_CONTENT**:
```json
{
  "detection_id": "uuid",
  "vehicle_lat": 43.7900,
  "vehicle_lon": -79.3140,
  "recording_timestamp": "18/01/2026 19:15:12",
  "class_name": "traffic light",
  "confidence": 0.85,
  "pixel_u": 1234.5,
  "pixel_v": 567.8
}
```

#### REF_OSM_NODES (Ground Truth)
```sql
CREATE TABLE REF_OSM_NODES (
    OSM_ID VARCHAR(50) PRIMARY KEY,
    OSM_TYPE VARCHAR(50),
    LATITUDE NUMBER(10, 7),
    LONGITUDE NUMBER(10, 7),
    TAGS VARIANT,
    UPLOADED_AT TIMESTAMP_NTZ,
    SOURCE_FILE VARCHAR(255)
);
```

**Spatial Queries**:
```sql
-- Convert lat/lon to GEOGRAPHY on-the-fly
SELECT 
    OSM_ID,
    ST_DISTANCE(
        TO_GEOGRAPHY(ST_MAKEPOINT(LONGITUDE, LATITUDE)),
        TO_GEOGRAPHY(ST_MAKEPOINT(-79.314, 43.790))
    ) AS distance_meters
FROM REF_OSM_NODES
WHERE distance_meters <= 50;
```

---

## üí∞ Cost Management

Current configuration limits spend to **$50 of $400 free trial credits**:

- ‚úÖ XSMALL warehouse (cheapest: $2/hour when running)
- ‚úÖ 60-second auto-suspend (stops billing after 1 min idle)
- ‚úÖ Resource monitor: Hard stop at 90% quota ($45)

**Monitor usage**:
```sql
-- Check resource monitor status
SHOW RESOURCE MONITORS;

-- View credit consumption
SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
WHERE WAREHOUSE_NAME = 'SENTINEL_WH'
ORDER BY START_TIME DESC
LIMIT 10;
```

---

## üõ†Ô∏è Troubleshooting

### Issue: "File not found: osm.xml"

**Solution**:
```bash
# Check if file exists
ls -la local-mvp/osm*.xml

# Download OSM data for your detection area
cd modules/data-modeling/scripts
python download_osm_from_detections.py
```

---

### Issue: "Snowflake connection failed"

**Solution**:
1. Check `.env` settings
2. Verify account identifier: `<locator>.<region>` (no https://)
3. Test credentials in Snowflake UI first

---

### Issue: "Connector showing 0 errors but no data in Snowflake"

**Root causes**:
1. **Error Tolerance = "all"** (connector silently swallows errors)
   - Fix: Change to **"none"** in connector settings
2. **Buffer not flushed** (only 26 messages sent)
   - Fix: Pause/Resume connector or wait 60-120 seconds
3. **Topic name mismatch** (producer sends to different topic)
   - Fix: Verify connector reads from `sentinel_map_detections`

---

## üìä Data Flow

```
Module A (Perception) ‚Üí CSV ‚Üí Module B (Kafka Producer)
                                        ‚Üì
                               Confluent Cloud
                                        ‚Üì
                          Snowpipe Streaming Connector
                                        ‚Üì
                        Snowflake STG_DETECTIONS (VARIANT)
                                        ‚Üì
local-mvp/osm.xml ‚Üí Python Script ‚Üí REF_OSM_NODES (GEOGRAPHY)
                                        ‚Üì
                          Spatial Join (ST_DISTANCE)
                                        ‚Üì
                            Audit Results (dbt)
```

---

## üó∫Ô∏è Next Steps

- [ ] Set up dbt Cloud for transformation layer
- [ ] Create data quality tests (confidence > 0.5, GPS in valid range)
- [ ] Build audit dashboard in Streamlit
- [ ] Implement H3/S2 spatial indexing for faster joins
- [ ] Add time-series analysis (detection trends over time)

---

## üìö Related Documentation

- [Module A (Perception)](../perception/README.md)
- [Module B (Ingestion)](../ingestion/README.md)
- [Script Documentation](scripts/README.md)
- [Main Project README](../../README.md)
- [Production Specs](../../docs/prod-specs-en.md)
